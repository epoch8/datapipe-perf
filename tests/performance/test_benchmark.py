"""
–ë–µ–Ω—á–º–∞—Ä–∫ —Ç–µ—Å—Ç –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π.

–ò–∑–º–µ—Ä—è–µ—Ç –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±—ä–µ–º–∞—Ö –¥–∞–Ω–Ω—ã—Ö.
–ü—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö –∫–æ–¥–∞ (–≤–µ—Ç–∫–∞—Ö) –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤: ~/.datapipe_benchmarks/

Workflow –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤–µ—Ç–æ–∫:
    1. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã –Ω–∞ –≤–µ—Ç–∫–µ master:
       git checkout master
       pytest tests/performance/test_benchmark.py -v -s

    2. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã –Ω–∞ –≤–µ—Ç–∫–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π:
       git checkout offset-optimization
       pytest tests/performance/test_benchmark.py -v -s

    3. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã –Ω–∞ –≤–µ—Ç–∫–µ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π:
       git checkout offset-with-status
       pytest tests/performance/test_benchmark.py -v -s

    4. –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
       pytest tests/performance/test_benchmark.py::test_compare_branches -v -s

–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ—Å—Ç—ã:
    - test_incremental_transform_performance: —Ç–µ—Å—Ç—ã –Ω–∞ 10K, 100K, 1M –∑–∞–ø–∏—Å–µ–π
    - test_incremental_update_throughput: —Ç–µ—Å—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏ initial/new
    - test_compare_branches: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–µ–∂–¥—É –≤–µ—Ç–∫–∞–º–∏
"""
import csv
import subprocess
import time
from datetime import datetime
from pathlib import Path
from typing import List

import pandas as pd
from pytest_cases import parametrize
from sqlalchemy import Column, Integer, String

from datapipe.compute import Catalog, Table, Pipeline, DatapipeApp, run_steps
from datapipe.datatable import DataStore
from datapipe.run_config import RunConfig
from datapipe.step.batch_transform import BatchTransform
from datapipe.store.database import DBConn, TableStoreDB


# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–≤–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞)
RESULTS_DIR = Path.home() / ".datapipe_benchmarks"
RESULTS_DIR.mkdir(exist_ok=True)


def get_git_branch():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è —Ç–µ–∫—É—â–µ–π git –≤–µ—Ç–∫–∏."""
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True,
            text=True,
            check=True,
        )
        return result.stdout.strip()
    except Exception:
        return "unknown"


def save_benchmark_result(
    test_name: str,
    dataset_params: str,
    step_1_time: float,
    step_2_time: float,
    step_3_time: float,
    step_4_time: float,
    initial_records: int,
    new_records: int,
):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ –≤ CSV —Ñ–∞–π–ª."""
    branch = get_git_branch()
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    csv_file = RESULTS_DIR / f"{test_name}_results.csv"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
    file_exists = csv_file.exists()

    with open(csv_file, "a", newline="") as f:
        writer = csv.writer(f)

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫, –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–æ–≤—ã–π
        if not file_exists:
            writer.writerow([
                "timestamp",
                "branch",
                "dataset_params",
                "initial_records",
                "new_records",
                "step_1_load_initial_sec",
                "step_2_transform_initial_sec",
                "step_3_load_incremental_sec",
                "step_4_transform_incremental_sec",
                "total_time_sec",
                "incremental_throughput_rec_per_sec",
            ])

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        total_time = step_1_time + step_2_time + step_3_time + step_4_time
        throughput = new_records / step_4_time if step_4_time > 0 else 0

        writer.writerow([
            timestamp,
            branch,
            dataset_params,
            initial_records,
            new_records,
            f"{step_1_time:.3f}",
            f"{step_2_time:.3f}",
            f"{step_3_time:.3f}",
            f"{step_4_time:.3f}",
            f"{total_time:.3f}",
            f"{throughput:.0f}",
        ])

    print(f"\n‚úì Results saved to: {csv_file}")


def compare_branches(test_name: str, branches: List[str] = None):
    """
    –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤ –º–µ–∂–¥—É –≤–µ—Ç–∫–∞–º–∏.

    Args:
        test_name: –ò–º—è —Ç–µ—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        branches: –°–ø–∏—Å–æ–∫ –≤–µ—Ç–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (None = –≤—Å–µ –≤–µ—Ç–∫–∏)
    """
    csv_file = RESULTS_DIR / f"{test_name}_results.csv"

    if not csv_file.exists():
        print(f"No results found for test: {test_name}")
        return

    # –ß–∏—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    df = pd.read_csv(csv_file)

    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤–µ—Ç–∫–∞–º, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
    if branches:
        df = df[df["branch"].isin(branches)]

    if df.empty:
        print(f"No results found for specified branches: {branches}")
        return

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –≤–µ—Ç–∫–∞–º –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –¥–∞—Ç–∞—Å–µ—Ç–∞
    print("\n" + "=" * 100)
    print(f"BENCHMARK COMPARISON: {test_name}")
    print("=" * 100)

    for dataset_params in df["dataset_params"].unique():
        df_dataset = df[df["dataset_params"] == dataset_params]

        print(f"\nüìä Dataset: {dataset_params}")
        print("-" * 100)

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—É—Å–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –≤–µ—Ç–∫–∏
        df_latest = df_dataset.sort_values("timestamp").groupby("branch").tail(1)

        # –í—ã–≤–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_data = []
        for _, row in df_latest.iterrows():
            comparison_data.append({
                "Branch": row["branch"],
                "Step 1 (load)": f"{row['step_1_load_initial_sec']:.3f}s",
                "Step 2 (transform)": f"{row['step_2_transform_initial_sec']:.3f}s",
                "Step 3 (load inc)": f"{row['step_3_load_incremental_sec']:.3f}s",
                "Step 4 (transform inc)": f"{row['step_4_transform_incremental_sec']:.3f}s",
                "Total": f"{row['total_time_sec']:.3f}s",
                "Throughput": f"{row['incremental_throughput_rec_per_sec']:.0f} rec/s",
            })

        df_comparison = pd.DataFrame(comparison_data)
        print(df_comparison.to_string(index=False))

        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –≤–µ—Ç–∫–∞–º–∏
        if len(df_latest) >= 2:
            print("\nüìà Performance difference:")

            branches_list = df_latest["branch"].tolist()
            base_branch = branches_list[0]

            for i in range(1, len(branches_list)):
                compare_branch = branches_list[i]

                base_row = df_latest[df_latest["branch"] == base_branch].iloc[0]
                compare_row = df_latest[df_latest["branch"] == compare_branch].iloc[0]

                print(f"\n  {compare_branch} vs {base_branch}:")

                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —à–∞–≥
                for col, label in [
                    ("step_1_load_initial_sec", "Step 1 (load initial)"),
                    ("step_2_transform_initial_sec", "Step 2 (transform initial)"),
                    ("step_3_load_incremental_sec", "Step 3 (load incremental)"),
                    ("step_4_transform_incremental_sec", "Step 4 (transform incremental)"),
                    ("total_time_sec", "Total time"),
                ]:
                    base_val = float(base_row[col])
                    compare_val = float(compare_row[col])

                    if base_val > 0:
                        diff_pct = ((compare_val - base_val) / base_val) * 100
                        speedup = base_val / compare_val if compare_val > 0 else 0

                        if diff_pct < 0:
                            print(f"    {label:30s}: {compare_val:.3f}s vs {base_val:.3f}s ‚Üí ‚úì {abs(diff_pct):.1f}% faster ({speedup:.2f}x)")
                        else:
                            print(f"    {label:30s}: {compare_val:.3f}s vs {base_val:.3f}s ‚Üí ‚úó {diff_pct:.1f}% slower ({speedup:.2f}x)")

                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º throughput
                base_throughput = float(base_row["incremental_throughput_rec_per_sec"])
                compare_throughput = float(compare_row["incremental_throughput_rec_per_sec"])

                if base_throughput > 0:
                    diff_pct = ((compare_throughput - base_throughput) / base_throughput) * 100
                    print(f"    {'Incremental throughput':30s}: {compare_throughput:.0f} vs {base_throughput:.0f} rec/s ‚Üí {'+' if diff_pct > 0 else ''}{diff_pct:.1f}%")

    print("\n" + "=" * 100)


def prepare_large_dataset(num_records: int, primary_key: str, start_idx=0):
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–∞—Ä—Ç–∏—è–º–∏.
    """
    batch_size = 50_000

    for i in range(0, num_records, batch_size):
        chunk_size = min(batch_size, num_records - i)

        data = pd.DataFrame({
            primary_key: [f"id_{(j + start_idx):010d}" for j in range(i, i + chunk_size)],
            "value": range(i, i + chunk_size),
            "category": [f"cat_{(j + start_idx) % 100}" for j in range(i, i + chunk_size)],
        })

        yield data


@parametrize('num_records', [10_000, 100_000, 1_000_000])
def test_incremental_transform_performance(num_records, dbconn: DBConn):
    """
    –ë–µ–Ω—á–º–∞—Ä–∫: –∏–∑–º–µ—Ä—è–µ—Ç –≤—Ä–µ–º—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.

    –®–∞–≥–∏:
    1. –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    2. –ü–µ—Ä–≤–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (full run)
    3. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
    4. –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–∏–∑–º–µ—Ä—è–µ—Ç—Å—è –≤—Ä–µ–º—è)
    """
    catalog = Catalog({
        'perf_source': Table(
            store=TableStoreDB(
                dbconn,
                "perf_source",
                data_sql_schema=[
                    Column("id", String, primary_key=True),
                    Column("value", Integer),
                    Column("category", String),
                ],
                create_table=True,
            )
        ),
        'perf_target': Table(
            store=TableStoreDB(
                dbconn,
                "perf_target",
                data_sql_schema=[
                    Column("id", String, primary_key=True),
                    Column("value", Integer),
                    Column("category", String),
                ],
                create_table=True,
            )
        )
    })
    ds = DataStore(dbconn, create_meta_table=True)

    def copy_transform(df):
        result = df.copy()
        result['value'] = result['value'] * 2
        return result

    pipeline = Pipeline([
        BatchTransform(
            copy_transform,
            inputs=['perf_source'],
            outputs=['perf_target'],
            transform_keys=['id'],
            chunk_size=100
        )
    ])
    app = DatapipeApp(ds, catalog, pipeline)

    # ========== –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ==========
    print(f"\n=== Step 1: Loading {num_records:,} initial records ===")
    source = catalog.get_datatable(ds, 'perf_source')
    target = catalog.get_datatable(ds, 'perf_target')

    start = time.time()
    for data_chunk in prepare_large_dataset(num_records, 'id'):
        source.store_chunk(data_chunk)
    load_initial_time = time.time() - start
    print(f"Step 1 time (store_chunk): {load_initial_time:.3f}s")

    # ========== –®–∞–≥ 2: –ü–µ—Ä–≤–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ==========
    print("\n=== Step 2: Initial processing (full run) ===")

    # –°–æ–∑–¥–∞–µ–º RunConfig —Å –≤–∫–ª—é—á–µ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    run_config = RunConfig(labels={"use_offset_optimization": True})

    start = time.time()
    run_steps(ds, app.steps, run_config=run_config)
    initial_transform_time = time.time() - start
    print(f"Step 2 time (transform): {initial_transform_time:.3f}s")

    initial_count = len(target.get_data())
    print(f"Records processed: {initial_count:,}")
    assert initial_count == num_records

    # ========== –®–∞–≥ 3: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π ==========
    new_records = 1_000
    print(f"\n=== Step 3: Adding {new_records:,} new records ===")

    start = time.time()
    for data_chunk in prepare_large_dataset(new_records, 'id', start_idx=num_records):
        source.store_chunk(data_chunk)
    load_incremental_time = time.time() - start
    print(f"Step 3 time (store_chunk): {load_incremental_time:.3f}s")

    # ========== –®–∞–≥ 4: –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ==========
    print("\n=== Step 4: Incremental processing ===")

    start = time.time()
    run_steps(ds, app.steps, run_config=run_config)
    incremental_transform_time = time.time() - start
    print(f"Step 4 time (transform): {incremental_transform_time:.3f}s")

    final_count = len(target.get_data())
    print(f"Final records: {final_count:,}")
    assert final_count == num_records + new_records

    # ========== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ==========
    print("\n=== Results ===")
    print(f"Initial dataset: {num_records:,} records")
    print(f"New records: {new_records:,}")
    print(f"")
    print(f"Step 1 (load initial): {load_initial_time:.3f}s")
    print(f"Step 2 (transform initial): {initial_transform_time:.3f}s")
    print(f"Step 3 (load incremental): {load_incremental_time:.3f}s")
    print(f"Step 4 (transform incremental): {incremental_transform_time:.3f}s")
    print(f"")
    print(f"Total time: {load_initial_time + initial_transform_time + load_incremental_time + incremental_transform_time:.3f}s")
    print(f"Incremental throughput: {new_records/incremental_transform_time:,.0f} rec/sec")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV
    save_benchmark_result(
        test_name="test_incremental_transform_performance",
        dataset_params=f"{num_records}",
        step_1_time=load_initial_time,
        step_2_time=initial_transform_time,
        step_3_time=load_incremental_time,
        step_4_time=incremental_transform_time,
        initial_records=num_records,
        new_records=new_records,
    )


@parametrize('initial_records, new_records', [
    (10_000, 1000),
    (100_000, 1000),
    (1_000_000, 1000),
])
def test_incremental_update_throughput(initial_records, new_records, dbconn: DBConn):
    """
    –ë–µ–Ω—á–º–∞—Ä–∫: –∏–∑–º–µ—Ä—è–µ—Ç throughput –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π.

    –®–∞–≥–∏:
    1. –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    2. –ü–µ—Ä–≤–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (full run)
    3. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
    4. –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–∏–∑–º–µ—Ä—è–µ—Ç—Å—è –≤—Ä–µ–º—è)
    """
    catalog = Catalog({
        'source': Table(
            store=TableStoreDB(
                dbconn,
                "source",
                data_sql_schema=[
                    Column("id", String, primary_key=True),
                    Column("value", Integer),
                    Column("category", String),
                ],
                create_table=True,
            )
        ),
        'target': Table(
            store=TableStoreDB(
                dbconn,
                "target",
                data_sql_schema=[
                    Column("id", String, primary_key=True),
                    Column("value", Integer),
                    Column("category", String),
                ],
                create_table=True,
            )
        )
    })
    ds = DataStore(dbconn, create_meta_table=True)

    def transform_func(df):
        result = df.copy()
        result['value'] = result['value'] * 2
        return result

    pipeline = Pipeline([
        BatchTransform(
            transform_func,
            inputs=['source'],
            outputs=['target'],
            transform_keys=['id'],
            chunk_size=100
        )
    ])
    app = DatapipeApp(ds, catalog, pipeline)

    source = catalog.get_datatable(ds, 'source')
    target = catalog.get_datatable(ds, 'target')

    # ========== –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ==========
    print(f"\n=== Step 1: Loading {initial_records:,} initial records ===")
    start = time.time()
    for data_chunk in prepare_large_dataset(initial_records, 'id'):
        source.store_chunk(data_chunk)
    load_initial_time = time.time() - start
    print(f"Step 1 time (store_chunk): {load_initial_time:.3f}s")

    # ========== –®–∞–≥ 2: –ü–µ—Ä–≤–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ==========
    print("\n=== Step 2: Initial processing (full run) ===")
    run_config = RunConfig(labels={"use_offset_optimization": True})

    start = time.time()
    run_steps(ds, app.steps, run_config=run_config)
    initial_transform_time = time.time() - start
    print(f"Step 2 time (transform): {initial_transform_time:.3f}s")

    initial_count = len(target.get_data())
    assert initial_count == initial_records
    print(f"Initial records processed: {initial_count:,}")

    # ========== –®–∞–≥ 3: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π ==========
    print(f"\n=== Step 3: Adding {new_records:,} new records ===")
    start = time.time()
    for data_chunk in prepare_large_dataset(new_records, 'id', start_idx=initial_records):
        source.store_chunk(data_chunk)
    load_incremental_time = time.time() - start
    print(f"Step 3 time (store_chunk): {load_incremental_time:.3f}s")

    # ========== –®–∞–≥ 4: –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ==========
    print("\n=== Step 4: Incremental processing ===")

    start = time.time()
    run_steps(ds, app.steps, run_config=run_config)
    incremental_transform_time = time.time() - start
    print(f"Step 4 time (transform): {incremental_transform_time:.3f}s")

    final_count = len(target.get_data())
    assert final_count == initial_records + new_records
    print(f"Final records: {final_count:,}")

    # ========== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ==========
    print("\n=== Results ===")
    print(f"Dataset: {initial_records:,} initial + {new_records:,} new = {final_count:,} total")
    print(f"")
    print(f"Step 1 (load initial): {load_initial_time:.3f}s")
    print(f"Step 2 (transform initial): {initial_transform_time:.3f}s")
    print(f"Step 3 (load incremental): {load_incremental_time:.3f}s")
    print(f"Step 4 (transform incremental): {incremental_transform_time:.3f}s")
    print(f"")
    print(f"Total time: {load_initial_time + initial_transform_time + load_incremental_time + incremental_transform_time:.3f}s")
    print(f"Incremental throughput: {new_records/incremental_transform_time:,.0f} rec/sec")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV
    save_benchmark_result(
        test_name="test_incremental_update_throughput",
        dataset_params=f"{initial_records}+{new_records}",
        step_1_time=load_initial_time,
        step_2_time=initial_transform_time,
        step_3_time=load_incremental_time,
        step_4_time=incremental_transform_time,
        initial_records=initial_records,
        new_records=new_records,
    )


def test_compare_branches():
    """
    –¢–µ—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –º–µ–∂–¥—É –≤–µ—Ç–∫–∞–º–∏.

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        pytest tests/performance/test_benchmark.py::test_compare_branches -v -s
    """
    print("\n" + "=" * 100)
    print("COMPARING BENCHMARK RESULTS")
    print("=" * 100)

    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ–±–æ–∏—Ö —Ç–µ—Å—Ç–æ–≤
    compare_branches("test_incremental_transform_performance")
    compare_branches("test_incremental_update_throughput")
